{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk_chapter7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q0zJE539xZuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import nltk, re, pprint\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "from nltk import book\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import abc\n",
        "from nltk.corpus import conll2000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCzvUDLQ4MgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1.\tThe IOB format categorizes tagged tokens as I, O and B. Why are three tags necessary? What problem would be caused if we used I and O tags exclusively?\n",
        "\tProvide a sentence to illustrate your point.\n"
      ]
    },
    {
      "metadata": {
        "id": "YBalC3QIyUQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "79daabf3-a27d-497a-abe2-ebffca5c3b4d"
      },
      "cell_type": "code",
      "source": [
        "def exercise1():\n",
        "    print(\"Why these tags are necessary?\")\n",
        "    print(\"The IOB tags are the standard file representation for chunk structures in a sentence. Also, it’s represent more than one chunk type, so long as the chunk do not overlap. A token is tagged as B if it marks the beginning of a chunk, subsequent tokens within the chunk are tagged I and all other tokens are tagged O.\")\n",
        "    print(\"\")\n",
        "    print(\"What problem would be caused if we used I and O tags exclusively?\")\n",
        "    print(\"If this then we will lose all the beginning tokens of the chunk, and this will arise the complexity for discovering the chunk structure.\")\n",
        "    print(\"Example-\")\n",
        "    print(\"('I', 'PRP','B-NP'), ('cracked', 'VBD','O'), ('the', 'DT','B-NP'), ('competition', 'NN','I-NP')]\")\n",
        "    print(\"If we skip the B tags here, it’s very tough to come with the sentence structure. \")\n",
        "\n",
        "exercise1()    "
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Why these tags are necessary?\n",
            "The IOB tags are the standard file representation for chunk structures in a sentence. Also, it’s represent more than one chunk type, so long as the chunk do not overlap. A token is tagged as B if it marks the beginning of a chunk, subsequent tokens within the chunk are tagged I and all other tokens are tagged O.\n",
            "\n",
            "What problem would be caused if we used I and O tags exclusively?\n",
            "If this then we will lose all the beginning tokens of the chunk, and this will arise the complexity for discovering the chunk structure.\n",
            "Example-\n",
            "('I', 'PRP','B-NP'), ('cracked', 'VBD','O'), ('the', 'DT','B-NP'), ('competition', 'NN','I-NP')]\n",
            "If we skip the B tags here, it’s very tough to come with the sentence structure. \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yXxUqV-P4RwW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "2.\tWrite tag patterns to match noun phrases containing plural head nouns, e.g. \"many/JJ researchers/NNS\", \"two/CD weeks/NNS\", \"both/DT new/JJ positions/NNS\". Extend the “grammar” defined in Example 3 (http://www.nltk.org/book/pylisting/code_chunkex.py)\n",
        "\tby your regular expressions. Redefine the “cp” object from this example to use your new \tgrammar. Use this object to parse sentenceSample defined as follows\n",
        "\n",
        "sentenceSample = [(\"Many\", \"JJ\"), (\"little\", \"JJ\"), (\"dogs\", \"NNS\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"), (\"cats\", \"NNS\")]\n",
        "\n",
        "\tReport your outcome.\n"
      ]
    },
    {
      "metadata": {
        "id": "lshQ_Eq9yrHp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "997bc6c2-2c74-4dd1-c9bf-2e069244b952"
      },
      "cell_type": "code",
      "source": [
        "def exercise2():\n",
        "    grammar = \"NP:{<DT>?<CD>?<JJ>*<NNS>}\"\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    sentenceSample = [(\"Many\", \"JJ\"), (\"little\", \"JJ\"), (\"dogs\", \"NNS\"), (\"barked\", \"VBD\"), (\"at\", \"IN\"),\n",
        "                      (\"cats\", \"NNS\")]\n",
        "    print(cp.parse(sentenceSample))\n",
        "    \n",
        "exercise2()"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S (NP Many/JJ little/JJ dogs/NNS) barked/VBD at/IN (NP cats/NNS))\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "4Rmi4jgb4kil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "3.\tCarry out the following evaluation tasks for the chunker you have developed in question 2. \n",
        "a)\tEvaluate your chunker on first 100 sentences from a chunked corpus nltk.corpus.conll2000, and report the precision, recall and F-measure. \n",
        "b)\tCompare the performance of your chunker to the baseline chunker discussed in the evaluation section of 3 (the very first chunker that does nothing). \n",
        "c)\tExtend the “grammar” of your chunker by at least one more regular expression. Give rationally behind your extension. See whether this extension allows you to boost the performance of your chunker. Evaluate your new chunker on 100 sentences from a chunked corpus nltk.corpus.conll2000, and report the precision, recall and F-measure. \n"
      ]
    },
    {
      "metadata": {
        "id": "jfxOYbdx4wNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b5cdc3d7-2a4a-4bda-e245-63f828bcd49f"
      },
      "cell_type": "code",
      "source": [
        "def exercise3():\n",
        "    print(\"part a\")\n",
        "    grammar = \"NP:{<DT>?<CD>?<JJ>*<NNS>}\"\n",
        "    cp = nltk.RegexpParser(grammar)\n",
        "    test = (conll2000.chunked_sents('test.txt',chunk_types = 'NP'))[:100]\n",
        "    print(cp.evaluate(test))\n",
        "    print(\"\")\n",
        "    print(\"part b\")\n",
        "    print(\"Chunk types: Nothing\")\n",
        "    cp1 = nltk.RegexpParser(\"\")\n",
        "    print(cp1.evaluate(test))\n",
        "    print(\"\")\n",
        "    print(\"Part c\")\n",
        "    grammar2 = r\"\"\"\n",
        "      NP: {<DT|PP\\$>?<JJ>*<NN>}\n",
        "          {<NNP>+}\n",
        "          {<DT>?<CD>?<JJ>*<NNS>}\n",
        "    \"\"\"\n",
        "    cp2 = nltk.RegexpParser(grammar2)\n",
        "    print(cp2.evaluate(test))\n",
        "\n",
        "\n",
        "\n",
        "exercise3()    "
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part a\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  45.7%%\n",
            "    Precision:     44.8%%\n",
            "    Recall:        12.5%%\n",
            "    F-Measure:     19.5%%\n",
            "\n",
            "part b\n",
            "Chunk types: Nothing\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  37.8%%\n",
            "    Precision:      0.0%%\n",
            "    Recall:         0.0%%\n",
            "    F-Measure:      0.0%%\n",
            "\n",
            "Part c\n",
            "ChunkParse score:\n",
            "    IOB Accuracy:  69.1%%\n",
            "    Precision:     45.3%%\n",
            "    Recall:        48.1%%\n",
            "    F-Measure:     46.7%%\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}