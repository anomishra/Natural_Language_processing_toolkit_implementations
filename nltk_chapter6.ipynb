{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nltk_chapter6.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "Q0zJE539xZuv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import division\n",
        "import nltk, re, pprint\n",
        "\n",
        "from urllib.request import urlopen\n",
        "\n",
        "from nltk import book\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "from nltk.corpus import brown\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import abc\n",
        "import random\n",
        "from nltk.corpus import names\n",
        "from nltk.classify import apply_features\n",
        "from nltk.corpus import movie_reviews\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import ppattach"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCzvUDLQ4MgS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "exercise 2.  Design at least 5 features and report what these features capture. Additionally, use three classifiers, namely, nltk.NaiveBayesClassifier, nltk.DecisionTreeClassifier,  nltk.MaxentClassifier. Compare the performance of the three classifiers by analyzing the accuracy. Report the accuracy of each classifier built using all of the features that you designed."
      ]
    },
    {
      "metadata": {
        "id": "YBalC3QIyUQ_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1989
        },
        "outputId": "93b4fe10-b245-43fe-ddac-322e75347995"
      },
      "cell_type": "code",
      "source": [
        "def exercise2():\n",
        "    labeled_names = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])\n",
        "    test, devtest, training = labeled_names[:500], labeled_names[500:1000], labeled_names[1000:]\n",
        "\n",
        "    def gender_features(name):\n",
        "        features = {}\n",
        "        features[\"first_letter\"] = name[0].lower()  # feature1\n",
        "        features[\"last_letter\"] = name[-1].lower()  # feature2\n",
        "        for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
        "            features[\"count({})\".format(letter)] = name.lower().count(letter)  # feature3\n",
        "            features[\"has({})\".format(letter)] = (letter in name.lower())  # feature4\n",
        "        features['vowels_len'] = len([word for word in name if word in 'aeiou'])  # feature5\n",
        "        return features\n",
        "\n",
        "    train_set = [(gender_features(n), g) for (n, g) in training]\n",
        "    devtest_set = [(gender_features(n), g) for (n, g) in devtest]\n",
        "    test_set = [(gender_features(n), gender) for (n, gender) in test]\n",
        "\n",
        "    print(\"Note: MaxentClassifier will train, which taked a while to run the program.\")\n",
        "    NaiveBayes = nltk.NaiveBayesClassifier.train(train_set)\n",
        "    DecisionTree = nltk.DecisionTreeClassifier.train(train_set)\n",
        "    Maxentropy = nltk.MaxentClassifier.train(train_set)\n",
        "\n",
        "    print(\"NaiveByesClassifier:\")\n",
        "    print(\"Performance, dev-test set: \",nltk.classify.accuracy(NaiveBayes,devtest_set))\n",
        "    print(\"Performance on test_set:\",nltk.classify.accuracy(NaiveBayes,test_set))\n",
        "    print(\"\")\n",
        "    print(\"DecisionTree:\")\n",
        "    print(\"Performance, dev-test set: \", nltk.classify.accuracy(DecisionTree, devtest_set))\n",
        "    print(\"Performance on test_set:\", nltk.classify.accuracy(DecisionTree, test_set))\n",
        "    print(\"\")\n",
        "    print(\"Maxentropy:\")\n",
        "    print(\"Performance, dev-test set: \", nltk.classify.accuracy(Maxentropy, devtest_set))\n",
        "    print(\"Performance on test_set:\", nltk.classify.accuracy(Maxentropy, test_set))\n",
        "\n",
        "\n",
        "exercise2()    "
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Note: MaxentClassifier will train, which taked a while to run the program.\n",
            "  ==> Training (100 iterations)\n",
            "\n",
            "      Iteration    Log Likelihood    Accuracy\n",
            "      ---------------------------------------\n",
            "             1          -0.69315        0.280\n",
            "             2          -0.50151        0.720\n",
            "             3          -0.48984        0.720\n",
            "             4          -0.47877        0.720\n",
            "             5          -0.46830        0.720\n",
            "             6          -0.45842        0.721\n",
            "             7          -0.44913        0.723\n",
            "             8          -0.44039        0.727\n",
            "             9          -0.43217        0.734\n",
            "            10          -0.42446        0.743\n",
            "            11          -0.41721        0.751\n",
            "            12          -0.41040        0.759\n",
            "            13          -0.40400        0.767\n",
            "            14          -0.39797        0.772\n",
            "            15          -0.39229        0.778\n",
            "            16          -0.38694        0.783\n",
            "            17          -0.38188        0.788\n",
            "            18          -0.37711        0.792\n",
            "            19          -0.37259        0.795\n",
            "            20          -0.36831        0.800\n",
            "            21          -0.36426        0.802\n",
            "            22          -0.36041        0.806\n",
            "            23          -0.35675        0.810\n",
            "            24          -0.35327        0.814\n",
            "            25          -0.34996        0.816\n",
            "            26          -0.34680        0.818\n",
            "            27          -0.34379        0.820\n",
            "            28          -0.34092        0.821\n",
            "            29          -0.33817        0.823\n",
            "            30          -0.33554        0.824\n",
            "            31          -0.33302        0.825\n",
            "            32          -0.33061        0.826\n",
            "            33          -0.32830        0.827\n",
            "            34          -0.32608        0.828\n",
            "            35          -0.32395        0.828\n",
            "            36          -0.32191        0.830\n",
            "            37          -0.31994        0.831\n",
            "            38          -0.31804        0.831\n",
            "            39          -0.31621        0.832\n",
            "            40          -0.31446        0.833\n",
            "            41          -0.31276        0.834\n",
            "            42          -0.31112        0.834\n",
            "            43          -0.30954        0.835\n",
            "            44          -0.30802        0.835\n",
            "            45          -0.30654        0.835\n",
            "            46          -0.30512        0.835\n",
            "            47          -0.30374        0.835\n",
            "            48          -0.30240        0.836\n",
            "            49          -0.30111        0.836\n",
            "            50          -0.29985        0.837\n",
            "            51          -0.29864        0.838\n",
            "            52          -0.29746        0.838\n",
            "            53          -0.29632        0.839\n",
            "            54          -0.29521        0.839\n",
            "            55          -0.29413        0.840\n",
            "            56          -0.29308        0.840\n",
            "            57          -0.29206        0.840\n",
            "            58          -0.29107        0.840\n",
            "            59          -0.29011        0.841\n",
            "            60          -0.28917        0.841\n",
            "            61          -0.28826        0.842\n",
            "            62          -0.28738        0.842\n",
            "            63          -0.28651        0.842\n",
            "            64          -0.28567        0.843\n",
            "            65          -0.28485        0.844\n",
            "            66          -0.28405        0.844\n",
            "            67          -0.28327        0.844\n",
            "            68          -0.28251        0.844\n",
            "            69          -0.28176        0.844\n",
            "            70          -0.28104        0.845\n",
            "            71          -0.28033        0.845\n",
            "            72          -0.27964        0.845\n",
            "            73          -0.27896        0.845\n",
            "            74          -0.27830        0.846\n",
            "            75          -0.27766        0.846\n",
            "            76          -0.27703        0.846\n",
            "            77          -0.27641        0.847\n",
            "            78          -0.27580        0.847\n",
            "            79          -0.27521        0.847\n",
            "            80          -0.27464        0.848\n",
            "            81          -0.27407        0.848\n",
            "            82          -0.27351        0.848\n",
            "            83          -0.27297        0.848\n",
            "            84          -0.27244        0.848\n",
            "            85          -0.27192        0.849\n",
            "            86          -0.27141        0.849\n",
            "            87          -0.27091        0.849\n",
            "            88          -0.27041        0.849\n",
            "            89          -0.26993        0.850\n",
            "            90          -0.26946        0.850\n",
            "            91          -0.26900        0.850\n",
            "            92          -0.26854        0.850\n",
            "            93          -0.26809        0.850\n",
            "            94          -0.26766        0.850\n",
            "            95          -0.26722        0.850\n",
            "            96          -0.26680        0.850\n",
            "            97          -0.26639        0.850\n",
            "            98          -0.26598        0.850\n",
            "            99          -0.26558        0.850\n",
            "         Final          -0.26518        0.851\n",
            "NaiveByesClassifier:\n",
            "Performance, dev-test set:  0.118\n",
            "Performance on test_set: 0.016\n",
            "\n",
            "DecisionTree:\n",
            "Performance, dev-test set:  0.102\n",
            "Performance on test_set: 0.008\n",
            "\n",
            "Maxentropy:\n",
            "Performance, dev-test set:  0.272\n",
            "Performance on test_set: 0.208\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yXxUqV-P4RwW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "exercise 4. pick any 5 features out of the computed 30 and describe their relevance."
      ]
    },
    {
      "metadata": {
        "id": "lshQ_Eq9yrHp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def exercise4():\n",
        "    documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for fileid in movie_reviews.fileids(category)]\n",
        "    all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "    word_features = list(all_words)\n",
        "\n",
        "    def document_features(document):\n",
        "        document_words = set(document)\n",
        "        features = {}\n",
        "        for word in word_features:\n",
        "            features['contains({})'.format(word)] = (word in document_words)\n",
        "        return features\n",
        "    print(\"This part will take a while to implement\")\n",
        "    featuresets = [(document_features(d), c) for (d, c) in documents]\n",
        "    train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "    print(\"The accuracy of classifier on test set is: \")\n",
        "    print(nltk.classify.accuracy(classifier, test_set))\n",
        "    classifier.show_most_informative_features(30)\n",
        "    print(\"The top five feature is: \")\n",
        "    classifier.show_most_informative_features(5)\n",
        "\n",
        "exercise4()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4Rmi4jgb4kil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "exercise 7.  Design at least 5 features and report what these features capture. Report the accuracy of your classifier. Place your classifier code into the report."
      ]
    },
    {
      "metadata": {
        "id": "jfxOYbdx4wNo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "307a9b66-29b3-4e01-8564-605f9242a0e4"
      },
      "cell_type": "code",
      "source": [
        "def exercise7():\n",
        "    posts = nltk.corpus.nps_chat.xml_posts()\n",
        "    p = posts[:10000]\n",
        "\n",
        "    def extract_features(post):\n",
        "        features = {}\n",
        "        features[\"suffix1\"] = post.text[-1:]\n",
        "        features[\"suffix2\"] = post.text[-2:]\n",
        "        features['vowels_len'] = len([word for word in post.text if word in 'aeiou'])\n",
        "        features[\"first_letter\"] = post.text[0]\n",
        "        for word in nltk.word_tokenize(post.text):\n",
        "            features['contains({})'.format(word.lower())] = True\n",
        "        return features\n",
        "\n",
        "    def Consecutive_classifier(posts):\n",
        "        history = []\n",
        "        for p in posts:\n",
        "            history.append((extract_features(p), p.get('class')))\n",
        "        return history\n",
        "\n",
        "    history = Consecutive_classifier(p)\n",
        "    test_size = int(len(history) * 0.1)\n",
        "    train_set, test_set = history[test_size:], history[:test_size]\n",
        "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "    print(\"The accuracy of consecutive classifier is- \", nltk.classify.accuracy(classifier, test_set))\n",
        "\n",
        "\n",
        "exercise7()    "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The accuracy of consecutive classifier is-  0.706\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Es8PQ1Ca597T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "4.\texercise 0 (0 is a dummy ). Word features can be very useful for performing document classification, since the words that appear in a document give a strong indication about what its semantic content is. However, many words occur very infrequently, and some of the most informative words in a document may never have occurred in our training data. One solution is to make use of a lexicon, which describes how different words relate to one another. Using WordNet lexicon, augment the movie review document classifier presented in Chapter 6 to use the following two features on the intersection of words appearing in a document to classify and words appearing in “word_features”:\n",
        "a)\tMake a binary feature which reports “KNOWN” if the word is found in WordNet (i.e. wn.synsets is non-empty) and “UNK” if it is not found.\n",
        "b)\tMake a lemma name feature. Select the first synset from wn.synsets and choose the first lemma name from synset.lemma_names as the appropriate lemma. Report “UNK” if it is not found.\n",
        "Report the accuracy of your classifier: use nltk.NaiveBayesClassifier, your test set should contain the first 100 instances in documents  defined as follows:\n",
        "\tfrom nltk.corpus import movie_reviews\n",
        "\t\tdocuments = [(list(movie_reviews.words(fileid)), category)\n",
        "\t\t     for category in movie_reviews.categories()\n",
        "\t\t     for fileid in movie_reviews.fileids(category)]\n",
        "The remaining instances in documents should be part of your training set.\n",
        "How does this accuracy compare to the accuracy of the classifier trained on the original feature set from the book? (Note that accuracy may not improve.) Why do you think you observe the behavior you observe? \n"
      ]
    },
    {
      "metadata": {
        "id": "jDlb237I6FbH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def exercise0():\n",
        "  try:\n",
        "    documents = [(list(movie_reviews.words(fileid)), category) for category in movie_reviews.categories() for  fileid in movie_reviews.fileids(category)]\n",
        "    all_words = nltk.FreqDist(w.lower() for w in movie_reviews.words())\n",
        "    word_features = list(all_words)\n",
        "\n",
        "    def check():\n",
        "      tag=[]\n",
        "      for c in range(39768):\n",
        "        for word_features[c] in word_features:\n",
        "            if not wn.synsets(word_features[c]):\n",
        "              tag.append(\"UNK\")\n",
        "            else:\n",
        "              tag.append(\"KNOWN\")\n",
        "        return tag\n",
        "\n",
        "    def check1():\n",
        "      tag=[]\n",
        "      for c in range(39768):\n",
        "          for word in word_features:\n",
        "              if not wn.synsets(word):\n",
        "                  tag.append(\"UNK\")\n",
        "              else:\n",
        "                  first_synsets = wn.synsets(word)[0]\n",
        "                  first_lemma = first_synsets.lemma_names()\n",
        "                  tag.append(first_lemma[0])\n",
        "      return tag\n",
        "\n",
        "    def document_features(document):\n",
        "        features = {}\n",
        "        features['binary({})'.format(word)] = check()\n",
        "        features['lemma({})'.format(word)] = check1()\n",
        "        return features\n",
        "\n",
        "\n",
        "    featuresets = [(document_features(d),c) for (d,c) in documents]\n",
        "    print(\"calculating\")\n",
        "    train_set, test_set = featuresets[100:], featuresets[:100]\n",
        "    classifier = nltk.NaiveBayesClassifier.train(train_set)\n",
        "    print(\"The accuracy of the classifier is: \",nltk.classify.accuracy(classifier, test_set))\n",
        "  except ValueError:\n",
        "      print(\"Taking long time!\")\n",
        "  except:\n",
        "      print(\"Taking long time!\")\n",
        "\n",
        "exercise0()  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BJFCS6C56Weg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "__Modified version:__ exercise 9.  Design at least 5 features and explain them.  Use nltk.NaiveBayesClassifier. Report the accuracy of your classifier built using all of the features that you designed.  Use show_most_inforamtive_feautures(5) functionality from the classifier to inspect the individual feature performance. Which of your features seem to be most influential? Note: http://www.nltk.org/howto/corpus.html#other-corpora provides a little more information on ppattach corpora than the textbook. Section 4 of the publication posted at https://works.bepress.com/yuliya_lierler/55/ starts by describing the dataset by Ratnaparkhi et al. (1994). This is exactly the dataset included in ppattach in NLTK.exercise 32.  Test your function on TestText with n=5, n=10, n=15, n=20, n=30. Does any of this number produce satisfactory/natural results? Include the output of your function for n=7. "
      ]
    },
    {
      "metadata": {
        "id": "F5w50mG26SSm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "68a50bdf-5a53-4810-c17e-6b9b3e4fd661"
      },
      "cell_type": "code",
      "source": [
        "def exercise9():\n",
        "    print('Extra Credit')\n",
        "    ppattach.attachments('training')\n",
        "    inst = ppattach.attachments('training')\n",
        "    nattach = [inst for inst in ppattach.attachments('training') if inst.attachment == 'N']\n",
        "\n",
        "\n",
        "    def get_features(inst):\n",
        "        features = {}\n",
        "        features['noun1'] = inst.noun1\n",
        "        features['noun2'] = inst.noun2\n",
        "        features['verb'] =  inst.verb\n",
        "        features['sent'] = inst.sent\n",
        "        features['attachment'] = inst.attachment\n",
        "        return features\n",
        "\n",
        "    feature_sets = [(get_features(inst), inst.prep) for inst in nattach]\n",
        "    train_Set, test_Set = feature_sets[2200:], feature_sets[:2200]\n",
        "\n",
        "    naiveBayesClassifier = nltk.NaiveBayesClassifier.train(train_Set)\n",
        "    print('Accuracy from using NaiveBayes classifier:',nltk.classify.accuracy(naiveBayesClassifier, test_Set))\n",
        "    print(\"\")\n",
        "    print(naiveBayesClassifier.most_informative_features(5))\n",
        "\n",
        "exercise9()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extra Credit\n",
            "Accuracy from using NaiveBayes classifier: 0.5686363636363636\n",
            "\n",
            "[('noun1', 'stake'), ('noun2', 'million'), ('verb', 'is'), ('noun1', '%'), ('noun1', 'questions')]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}